{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "import plotly.express as px\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bots=[]\n",
    "with open(\"../data/for_analysis/bot_fake_ids_2.txt\",\"r\") as f:\n",
    "    for line in f:\n",
    "        bots.append(int(line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\"../data/for_analysis/data0.pkl\")\n",
    "data = data.loc[data.lang==\"en\"]\n",
    "data = data.loc[data.text_rt != \"\"]\n",
    "data = data.loc[~data.author_id.isin(bots)]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mentions_and_links(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = \"\" if (t.startswith('@') or t.startswith('#') ) and len(t) > 1 else t\n",
    "        new_text.append(t)\n",
    "\n",
    "    new_text = re.sub(r'http\\S+', '', \" \".join(new_text))\n",
    "    return new_text\n",
    "\n",
    "data[\"prep\"] = data.text_rt.apply(remove_mentions_and_links)\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "data[\"sentiment\"] = [analyzer.polarity_scores(text) for text in data.prep]\n",
    "\n",
    "def categorize(scores_dict):\n",
    "    compound=scores_dict['compound']\n",
    "    if compound>=0.05:\n",
    "        return \"positive\"\n",
    "    elif compound <=-0.05:\n",
    "        return \"negative\"\n",
    "    elif (compound > -0.05) and(compound < 0.05):\n",
    "        return \"neutral\"\n",
    "\n",
    "data[\"score\"] = data[\"sentiment\"].apply(categorize)\n",
    "data['compound'] = [i['compound'] for i in data[\"sentiment\"]]\n",
    "\n",
    "data.to_pickle(\"../data/for_analysis/data1_no_bots.pkl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\"../data/for_analysis/data1_no_bots.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_only(text):\n",
    "    regex=u\"[a-zA-Z\\.\\?!:;\\-']+\"\n",
    "    regex = re.compile(regex)\n",
    "    return \" \".join(regex.findall(text))\n",
    "\n",
    "def basic_preprocessing(texts):\n",
    "    texts = texts.str.replace(\"https:\",\"\")\n",
    "    texts = texts.str.replace(\"t.co\",\"\")\n",
    "    # remove tel and email\n",
    "    texts = texts.str.replace(\"<email>\",\"\")\n",
    "    texts = texts.str.replace(\"<tel>\",\"\")\n",
    "    texts = texts.str.replace(\"<link>\",\"\")\n",
    "    texts = [re.sub(r'anon\\d*',\"\",t) for t in texts]\n",
    "    # Remove new line characters\n",
    "    texts = [re.sub('\\s+', ' ', t) for t in texts]\n",
    "    # Remove single quotes\n",
    "    texts = [re.sub(\"\\'\", \"\", sent) for sent in texts]\n",
    "    # remove some punctuation and numbers, emoji\n",
    "    texts = [words_only(t.lower()).strip() for t in texts]\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.prep = basic_preprocessing(data.prep)\n",
    "data.prep = data.prep.str.replace(\"chatgpt\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[data.prep !=\"\"]\n",
    "print(\"Non-empty tweets in English: \", data.shape[0])\n",
    "\n",
    "sparking = data.loc[data.conversation_id == data.id]\n",
    "print(\"Non-conversation tweets: \", sparking.shape[0])\n",
    "\n",
    "convs = data.loc[data.conversation_id != data.id]\n",
    "print(\"Conversation tweets: \", convs.shape[0])\n",
    "\n",
    "data_no_retweets = sparking.loc[~sparking.text.str.startswith(\"RT \")]\n",
    "#print(\"Non-conversational non-retweets: \", data_no_retweets.loc[~data_no_retweets.author_id.isin(bots)].shape[0])\n",
    "print(\"Non-conversation non-retweets: \", data_no_retweets.shape[0])\n",
    "\n",
    "retweets = sparking.loc[sparking.text.str.startswith(\"RT \")]\n",
    "#print(\"Non-conversational retweets: \", retweets.loc[~retweets.author_id.isin(bots)].shape[0])\n",
    "print(\"Non-conversation retweets: \", retweets.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from flair.embeddings import TransformerDocumentEmbeddings\n",
    "\n",
    "\n",
    "def topic_modelling(text_prep):\n",
    "    docs=list(text_prep)\n",
    "    ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "    #roberta = TransformerDocumentEmbeddings(\"cardiffnlp/twitter-roberta-base-sep2022\")\n",
    "    #model=BERTopic(embedding_model=roberta,ctfidf_model=ctfidf_model,n_gram_range = (1,2),verbose=True,language='English',low_memory=True,min_topic_size=100)\n",
    "    model=BERTopic(ctfidf_model=ctfidf_model,n_gram_range = (1,2),verbose=True,language='English',low_memory=True,min_topic_size=500)\n",
    "    topics=model.fit_transform(docs)\n",
    "    new_topics = model.reduce_outliers(docs,topics[0])\n",
    "    model.update_topics(docs, topics=new_topics)\n",
    "    topic_info=model.get_topic_info()\n",
    "    return model.topics_, topic_info,model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_retweets[\"topics\"],topic_info,model = topic_modelling(data_no_retweets[\"prep\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_retweets.to_pickle(\"../data/for_analysis/topics.pkl\")\n",
    "topic_info.to_excel(\"../data/for_analysis/topic_info_bert_sparking.xlsx\")\n",
    "data_no_retweets.loc[:,[\"text_rt\",\"text\",\"date\",\"topics\",\"sentiment\"]].sample(10000).to_excel(\"../data/for_analysis/sample_topic_bert_sparking.xlsx\")\n",
    "model.save(\"../data/for_analysis/model_bertopic_sparking_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "topic_labels = model.topic_labels_.values()\n",
    "sim_matrix = cosine_similarity(model.topic_embeddings_)\n",
    "sim_df = pd.DataFrame(data=sim_matrix,   \n",
    "             index=topic_labels,    \n",
    "               columns=topic_labels)\n",
    "\n",
    "sim_df.to_excel(\"../analysis/topics_cosine_sim.xlsx\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extrapolation on the non-conversational retweets only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_retweets = pd.read_pickle(\"../data/for_analysis/topics.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_retweets = data_no_retweets.loc[~data_no_retweets.author_id.isin(bots)]\n",
    "data_no_retweets=data_no_retweets.reset_index(drop=True)\n",
    "print(data_no_retweets.shape)\n",
    "data_no_retweets = data_no_retweets.drop_duplicates(subset = [\"text_rt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweets = retweets.loc[~retweets.author_id.isin(bots)]\n",
    "retweets=retweets.reset_index(drop=True)\n",
    "retweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(retweets,data_no_retweets.loc[:,[\"text_rt\",\"topics\"]],left_on=\"text_rt\",right_on = \"text_rt\",how='left')\n",
    "merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(merged.topics.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweets_found = merged.loc[~merged.topics.isnull()].copy()\n",
    "retweets_not_found = merged.loc[merged.topics.isnull()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics,_ = model.transform(retweets_not_found.prep.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweets_not_found[\"topics\"] = topics\n",
    "retweets = pd.concat([retweets_found,retweets_not_found],axis=0)\n",
    "retweets.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging topics and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_retweets = pd.read_pickle(\"../data/for_analysis/topics.pkl\")\n",
    "data_no_retweets = data_no_retweets.loc[~data_no_retweets.author_id.isin(bots)]\n",
    "data_all = pd.concat([retweets,data_no_retweets],axis=0)\n",
    "data_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics ={\n",
    "    \"AI in general\":[1,7,15,22,74,78,84],\n",
    "    \"Impact on search engines\":\t[2,25,60,63,70,75,87,119],\n",
    "    \"Education\":[4,6,31,34,44,45,73,77,82,102,122],\n",
    "    \"Impact on art (poems and lyrics, movies,books)\":\t[94,80,9,10,19,37],\n",
    "    \"Openai and its Investors and products (Microsoft, Musk)\":\t[127,114,97,90,59,3,17,40,50],\n",
    "    \"Cybersecurity (writing malware)\":\t[12,30],\n",
    "    \"Programming\":\t[16,57,67,111,128],\n",
    "    \"Digital content generation(podcasts,youtube scripts,quizzes)\":[85,81,38,39,43,54],\n",
    "    \"Access and price\":\t[14,23,55,65,66,104,108,110,117],\n",
    "    \"Business routine\":\t[18,79,89,103,115],\n",
    "    \"Social events on ChatGPT, discussion on media\": [56,64],\n",
    "    \"Politics\":\t[5],\n",
    "    \"Recipes\":\t[20],\n",
    "    \"Legal issues\":\t[35,72],\n",
    "    \"Calculator, math\":\t[32],\n",
    "    \"Job loss\":\t[29],\n",
    "    \"ChatGPT's competitors\":[126],\n",
    "    \"LLM technology\":\t[21,24,51,52,58,76,95,105,125],\n",
    "    \"Text to audio/voice\":[62,71],\n",
    "    \"Translation\":\t[100],\n",
    "    \"Healthcare\":\t[26,69,96],\n",
    "    \"Criticism in terms of ethics\":[68,116],\n",
    "    \"Q&A platforms\":\t[107],\n",
    "    \"Questions examples, prompt engineering\":[0,27,33,41,101,112],\n",
    "    \"Entertainment\":[42,109,113,120,123],\n",
    "    \"Robots\":[53],\n",
    "    \"Christmas\":[47],\n",
    "    \"Finance\":\t[11,13,36],\n",
    "    \"Climate change\":[86],\n",
    "    \"Religion, sermons\":[49],\n",
    "    \"Terrifying, insane\":\t[48,61,93],\n",
    "    \"Gender\"\t:[91],\t\n",
    "    \"ChatGPT on social media\": [8,106,118,121,124],\n",
    "    \"Real estate\":\t[99],\n",
    "    \"Sport\":[28],\n",
    "    \"Quantum computing\":[98],\n",
    "    \"Spam\":[13],\n",
    "    \"Making money with ChatGPT\":[88]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_flipped = {}\n",
    "for key,item in topics.items():\n",
    "    for i in item:\n",
    "        topics_flipped[i] = key\n",
    "        \n",
    "data_all[\"topics_general\"] = data_all['topics'].replace(topics_flipped)\n",
    "data_all.loc[~data_all.topics_general.isin(topics.keys()),\"topics_general\"]=-1\n",
    "data_all = data_all.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all['total']=1\n",
    "volumes = data_all.loc[:,[\"topics_general\",\"total\"]].groupby(\"topics_general\").sum()\n",
    "volumes = volumes.sort_values(\"total\",ascending=False).reset_index()\n",
    "volumes[\"precent\"] = volumes[\"total\"]/sum(volumes[\"total\"])*100\n",
    "volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volumes.to_excel(\"../analysis/topics_volume_final_no_bots.xlsx\")\n",
    "#data_all.to_pickle(\"../data/for_analysis/data2.pkl\")\n",
    "data_all.to_pickle(\"../data/for_analysis/data2_no_bots.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"total\"]=1\n",
    "sent_df = data.loc[(data.topics_general !=\"Outliers\")&(data.topics_general !=\"Spam\"),[\"date\",\"total\",\"score\"]].groupby([\"date\",\"score\"]).sum().reset_index()\n",
    "sent_df[\"perc\"] = sent_df['total'] / sent_df.groupby('date')['total'].transform('sum')\n",
    "sent_df.columns = [\"Date\",\"Sentiment\",\"Tweets per Day\",\"Percent\"]\n",
    "sent_df.to_excel(\"../analysis/ALL_sentiment_graph_no_outliers.xlsx\")\n",
    "fig = px.line(sent_df, x=\"Date\", y=\"Percent\", title='Dynamics of sentiment',color=\"Sentiment\",\n",
    "                template=\"plotly_white\", color_discrete_sequence=[ 'red','grey',\"green\"],\n",
    "                 width=800, height=600)\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## + Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = pd.read_pickle(\"../data/for_analysis/data1.pkl\")\n",
    "conv = conv.loc[conv.conversation_id != conv.id]\n",
    "conv = conv.loc[~conv.author_id.isin(bots)]\n",
    "conv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "dict_conv_avg = {}\n",
    "conv[\"topics_general\"] = -1\n",
    "\n",
    "print(conv.shape)\n",
    "conv = conv.loc[~conv.author_id.isin(bots)]\n",
    "print(conv.shape)\n",
    "\n",
    "for topic_name in data_all.topics_general.unique():\n",
    "    conv_ids_unique = list(data_all.loc[data_all.topics_general == topic_name]['conversation_id'].unique())\n",
    "    conv.loc[conv.conversation_id.isin(conv_ids_unique) , \"topics_general\"] = topic_name\n",
    "    conv_ids_topic = conv.loc[conv.conversation_id.isin(conv_ids_unique)][\"conversation_id\"].to_list()\n",
    "    dict_conv_avg[topic_name] =np.mean(list(Counter(conv_ids_topic).values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volumes[\"average_conv\"] = volumes.topics_general.replace(dict_conv_avg)\n",
    "volumes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volumes.to_excel(\"../analysis/topics_volume_final_no_bots.xlsx\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_topics = pd.read_pickle(\"../data/for_analysis/data2_no_bots.pkl\")\n",
    "\n",
    "# merge topics with text in data1_no_bots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = data_topics.loc[data_topics.topics_general==\"Spam\",\"edit_history_tweet_ids\"].to_list()\n",
    "spam = [str(spam) for spam in spam ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_fake_conv_ids = data.loc[data.edit_history_tweet_ids.astype(str).isin(spam),\"conversation_id\"].to_list()\n",
    "spam_fake_conv_ids = set(spam_fake_conv_ids)\n",
    "len(spam_fake_conv_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/spam_fake_conv_ids.txt\",\"w\") as f:\n",
    "    for conv_id in spam_fake_conv_ids:\n",
    "        f.write(str(conv_id))\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[~data.conversation_id.isin(spam_fake_conv_ids)]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"total\"]=1\n",
    "sent_df = data.loc[:,[\"date\",\"total\",\"score\"]].groupby([\"date\",\"score\"]).sum().reset_index()\n",
    "sent_df[\"perc\"] = sent_df['total'] / sent_df.groupby('date')['total'].transform('sum')\n",
    "sent_df.columns = [\"Date\",\"Sentiment\",\"Tweets per Day\",\"Percent\"]\n",
    "\n",
    "sent_df.to_excel(\"../analysis/ALL_sentiment_graph.xlsx\")\n",
    "\n",
    "fig = px.line(sent_df, x=\"Date\", y=\"Percent\", title='Dynamics of sentiment',color=\"Sentiment\",\n",
    "                template=\"plotly_white\", color_discrete_sequence=[ 'red','grey',\"green\"],\n",
    "                 width=800, height=600)\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset of Tweets on Education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edu = data_all.loc[data_all.topics_general == \"Education\"]\n",
    "edu = edu.drop([\"topics\",\"total\"],axis=1)\n",
    "\n",
    "edu[\"spark\"] = 1\n",
    "conv_edu = conv.loc[conv.topics_general == \"Education\"]\n",
    "conv_edu[\"spark\"]=0\n",
    "print(conv_edu.shape)\n",
    "edu = pd.concat([edu,conv_edu],axis=0)\n",
    "edu = edu.reset_index(drop=True)\n",
    "edu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edu.to_pickle(\"../data/for_analysis/edu_no_bots.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c498fff0e646d7d7e47743c6b62b2b1749eb16b128d528401df01971235654c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
