{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook:\n",
    "\n",
    "1. Multiple datasets with main tweets collected by keyword search and corresponding conversations are concatenated. \n",
    "\n",
    "    There are 2 datasets for the tweets downloaded by keyword search (ChatGPT). 1. data_data.json is from 30 Nov 2022 to 15 Jan 2023. 2. data_31jan_data is from 15 Jan 2023 to 31 Jan 2023.\n",
    "\n",
    "    There are 3 datasets for converstations: 1. conv_data.json contains the conversation tweets from 30 Nov 2022 to 15 Jan 2023 which were replies to the sparking tweets that mentioned ChatGPT. 2. conv_extended_data.json has the conversation tweets from 30 Nov 2022 to 15 Jan 2023 which were a part of conversation where at least one reply mentioned ChatGPT (sparking tweets did not necessarily mention ChatGPT). 3. conv_31jan_data.json contains the conversations from 16 Jan 2023 to 31 Jan 2023.\n",
    "\n",
    "2. Retweets in English were augmented with full text.\n",
    "\n",
    "3. Bots detection based on the number of tweets (more than 1000) and account description.\n",
    "\n",
    "4. Data anonymization (author_id, tweet_id and mentions are replaced with random numbers).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_15jan = pd.read_json(\"../data/for_analysis/jsons_concatenated/data_data.json\")\n",
    "conv_15jan_spark = pd.read_json(\"../data/for_analysis/jsons_concatenated/conv_data.json\")\n",
    "conv_15jan_reply = pd.read_json(\"../data/for_analysis/jsons_concatenated/conv_extended_data.json\")\n",
    "data_31jan = pd.read_json('../data/for_analysis/jsons_concatenated/data_31jan_data.json')\n",
    "conv_31jan = pd.read_json('../data/for_analysis/jsons_concatenated/conv_31jan_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_date(date_column,conv=False):\n",
    "    if conv:\n",
    "        return pd.to_datetime(date_column, utc=True).dt.date\n",
    "    else:\n",
    "        return pd.to_datetime(date_column).dt.date\n",
    "\n",
    "conv = pd.concat([conv_15jan_spark,conv_15jan_reply,conv_31jan,data_31jan],axis=0)\n",
    "conv=conv.reset_index(drop=True)\n",
    "data = data_15jan.copy()\n",
    "\n",
    "data['date'] = to_date(data['created_at'])\n",
    "conv['date'] = to_date(conv['created_at'],conv=True)\n",
    "\n",
    "data = pd.concat([data,conv],axis=0)\n",
    "data=data.reset_index(drop=True)\n",
    "conv_15jan_spark,conv_15jan_reply,conv_31jan,data_31jan,data_15jan = None,None,None,None,None\n",
    "\n",
    "data.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Handling retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates(subset = [\"text\",\"id\",\"conversation_id\"])\n",
    "data=data.reset_index(drop=True)\n",
    "retweets = data.loc[(data.text.str.startswith(\"RT \")) & (~data.referenced_tweets.isnull())].copy()\n",
    "originals = data.loc[(~data.text.str.startswith(\"RT \")) | (data.referenced_tweets.isnull()),[\"id\",\"text\"]].copy()\n",
    "originals = originals.set_index(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "originals = originals.to_dict()['text']\n",
    "len(originals.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_rt = []\n",
    "not_found = []\n",
    "repl_retw = []\n",
    "\n",
    "for r in retweets[\"referenced_tweets\"]:\n",
    "    if len(r)==1:\n",
    "        try:\n",
    "            text_rt.append(originals[int(r[0]['id'])])\n",
    "            repl_retw.append(r[0]['type'])\n",
    "            not_found.append(None)\n",
    "        except:\n",
    "            text_rt.append(None)\n",
    "            repl_retw.append(r[0]['type'])\n",
    "            not_found.append(int(r[0]['id']))\n",
    "    else:\n",
    "        try:\n",
    "            text_rt.append(originals[int(r[0]['id'])])\n",
    "            repl_retw.append(r[0]['type'])\n",
    "            not_found.append(None)\n",
    "        except:\n",
    "            text_rt.append(None)\n",
    "            repl_retw.append(r[0]['type'])\n",
    "            not_found.append(int(r[0]['id']))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweets[\"text_rt\"] = text_rt \n",
    "retweets[\"repl_retw\"] = repl_retw\n",
    "retweets[\"not_found\"] = not_found "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(~retweets.not_found.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweets in ENGLISH only\n",
    "missing = retweets.loc[(~retweets[\"not_found\"].isnull()) & (retweets[\"lang\"]==\"en\"), \"not_found\"].to_list()\n",
    "print(len(missing))\n",
    "missing = set(missing)\n",
    "print(len(missing))\n",
    "with open(\"../data/missing_retweets.txt\",\"w\") as f:\n",
    "    for tweet_id in missing:\n",
    "        f.write(str(int(tweet_id)))\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/missing_retweets.txt\",\"r\") as f:\n",
    "    missing = f.readlines()\n",
    "missing = [int(k.rstrip('\\n')) for k in missing]\n",
    "len(missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweets = retweets.drop(\"repl_retw\",axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data augmentation with missing tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "path='../data/missing_tweets/'\n",
    "files = os.listdir(path)\n",
    "\n",
    "count=0\n",
    "for file in tqdm(files):\n",
    "    with open(path+file, 'r',encoding=\"utf-8\") as f:\n",
    "        temp = json.load(f)\n",
    "        if count==0:\n",
    "            mis = pd.json_normalize(temp)\n",
    "            count+=1\n",
    "        else:\n",
    "            df = pd.json_normalize(temp)\n",
    "            mis = pd.concat([mis,df],axis=0)\n",
    "mis = mis.reset_index(drop=True)\n",
    "mis.to_json(\"../data/for_analysis/missing_retweets.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mis = pd.read_json(\"../data/for_analysis/missing_retweets.json\")\n",
    "mis = mis.loc[:,[\"id\",\"text\"]]\n",
    "mis.columns = [\"id\",\"text_rt\"]\n",
    "mis.id = mis.id.astype('int64')\n",
    "\n",
    "originals = data.loc[(~data.text.str.startswith(\"RT \")) | (data.referenced_tweets.isnull())].copy()\n",
    "originals[\"text_rt\"] = originals[\"text\"]\n",
    "originals[\"not_found\"] = -1\n",
    "\n",
    "retweets.loc[retweets.not_found.isnull(),\"not_found\"] = -1\n",
    "retweets.not_found = retweets.not_found.astype('int64')\n",
    "\n",
    "retweets_not_found = retweets.loc[retweets.not_found != -1].copy()\n",
    "retweets_found = retweets.loc[retweets.not_found == -1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(mis.id))\n",
    "print(len(mis.id.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(retweets_not_found.not_found))\n",
    "print(len(retweets_not_found.not_found.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(retweets_not_found, mis, left_on = \"not_found\", right_on =\"id\", how=\"left\")\n",
    "merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged[\"text_rt\"] = merged[\"text_rt_y\"]\n",
    "merged[\"id\"] = merged[\"id_x\"]\n",
    "merged=merged.drop([\"id_x\",\"id_y\",\"text_rt_y\",\"text_rt_x\"],axis=1)\n",
    "sum(~merged[\"text_rt\"].isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweets_augm = pd.concat([retweets_found,merged],axis=0)\n",
    "retweets_augm.shape[0] == retweets.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augm = pd.concat([originals, retweets],axis=0)\n",
    "data.shape[0] == data_augm.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(data_augm.text_rt.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augm = data_augm.reset_index(drop=True)\n",
    "data_augm.to_pickle(\"../data/for_analysis/data_augm.pkl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Bot Detection by Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\"../data/for_analysis/data_augm.pkl\")\n",
    "print(data.shape)\n",
    "data = data.drop_duplicates(subset=['text','id'])\n",
    "data = data.reset_index(drop=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data.author_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_eng = data.loc[data.lang==\"en\",\"author_id\"].unique()\n",
    "len(authors_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['total']=1\n",
    "potential_bots = data.loc[:,[\"author_id\",\"total\"]].groupby(\"author_id\").sum()\n",
    "potential_bots.sort_values(\"total\",ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_bots.loc[potential_bots.total>1000].total.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_ids = potential_bots.loc[potential_bots.total>10].index.to_list()\n",
    "print(\"Number of accounts that tweeted more than 10 tweets: \", len(author_ids))\n",
    "\n",
    "with open(\"../data/potential_bots.txt\",\"w\") as f:\n",
    "    for author_id in author_ids:\n",
    "        f.write(str(int(author_id)))\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "path='../data/potential_bots/'\n",
    "files = os.listdir(path)\n",
    "\n",
    "count=0\n",
    "for file in tqdm(files):\n",
    "    with open(path+file, 'r',encoding=\"utf-8\") as f:\n",
    "        temp = json.load(f)\n",
    "        if count==0:\n",
    "            users = pd.json_normalize(temp)\n",
    "            count+=1\n",
    "        else:\n",
    "            df = pd.json_normalize(temp)\n",
    "            users = pd.concat([users,df],axis=0)\n",
    "users = users.reset_index(drop=True)\n",
    "users.to_json(\"../data/for_analysis/users.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.read_json(\"../data/for_analysis/users.json\")\n",
    "users.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users[\"bot\"] = [re.findall(r\"(?<![a-zA-Z])bot\\W\",descr.lower()) for descr in users.description]\n",
    "users[\"bot\"] = [1 if lst else 0 for lst in users[\"bot\"]]\n",
    "sum(users.bot==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users[\"app\"] = [re.findall(r\"app(?=[^a-zA-Z])\",scr.lower()) for scr in users.screen_name]\n",
    "users[\"app\"] = [1 if lst else 0 for lst in users[\"app\"]]\n",
    "sum(users[\"app\"]==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users[\"bot\"] = 1*((users[\"bot\"] + users[\"app\"])>0)\n",
    "sum(users[\"bot\"]==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bots = users.loc[users[\"bot\"]==1]\n",
    "bots.to_json(\"../data/for_analysis/bots_2.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bots_active = potential_bots.loc[potential_bots.total>1000].index.to_list()\n",
    "len(bots_active)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bots = set(bots_active+users.loc[users[\"bot\"]==1,\"id\"].to_list())\n",
    "len(bots)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Anonymization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"total\"]=1\n",
    "print(\"The number of tweets: \", data.shape[0])\n",
    "print(\"The number of unique users: \", len(data.author_id.unique()))\n",
    "print(\"The number of conversation ids: \", sum(data.conversation_id.value_counts()>0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1. Twitter-assigned tweet, user, and conversation identifiers with random, numeric, and anonymous identifiers.\n",
    "ids_tweets = set(data['id'].to_list()+data[\"conversation_id\"].to_list())\n",
    "ids_authors = set(data['author_id'].to_list())\n",
    "\n",
    "replace_tweets = dict(zip(ids_tweets,range(len(ids_tweets))))\n",
    "replace_authors = dict(zip(ids_authors,range(len(ids_authors))))\n",
    "\n",
    "data['author_id_real'] = data['author_id']\n",
    "data['id_real'] = data['id']\n",
    "data[\"conversation_id_real\"] = data['conversation_id']\n",
    "\n",
    "\n",
    "data['author_id'] = [replace_authors[i] for i in data['author_id']]\n",
    "data['id'] = [replace_tweets[i] for i in data['id']]\n",
    "data[\"conversation_id\"] = [replace_tweets[i] for i in data['conversation_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(data['text_rt'].isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(~data.text_rt.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_rt'] = data['text_rt'].fillna(\"\")\n",
    "data['text'] = data['text'].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2. Replaced email addresses and phone numbers with placeholders (e.g., <tel> for telephone numbers). \n",
    "def replace_email_phone_links(text):\n",
    "    text = re.sub('([A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,})', \"<email>\", text)\n",
    "    text = re.sub('(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})',\"<tel>\",text)\n",
    "    text = re.sub('https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', \"<link>\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "data['text_rt'] = [replace_email_phone_links(t) for t in data['text_rt']]\n",
    "data['text'] = [replace_email_phone_links(t) for t in data['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(data['text_rt'].str.contains(\"<tel>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(data['text_rt'].str.contains(\"<email>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(data['text_rt'].str.contains(\"<link>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3. Replaced all mentions of usernames in tweets with their new anonymous user identifier (e.g., “@18415125”). \n",
    "unique_mentions = [re.findall(r'@\\S+', text) for text in data.text.to_list() + data.text_rt.to_list()]\n",
    "unique_mentions = set([m for ml in unique_mentions for m in ml])\n",
    "replacement = [\"anon\"+str(i) for i in range(len(unique_mentions))]\n",
    "replace_authors_name = dict(zip(unique_mentions, replacement))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_authors(text):\n",
    "    mentions = re.findall(r'@\\S+', text)\n",
    "    if mentions:\n",
    "        for mention in mentions:\n",
    "            text = text.replace(mention,replace_authors_name[mention])\n",
    "        return text\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "data['text'] = [replace_authors(t) for t in data['text']]\n",
    "print(\"text is finished\")\n",
    "data['text_rt'] = [replace_authors(t) for t in data['text_rt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_fake_ids = set(data.loc[data.author_id_real.isin(bots),\"author_id\"].to_list())\n",
    "with open(\"../data/for_analysis/bot_fake_ids_2.txt\",\"w\") as f:\n",
    "    for i in bot_fake_ids:\n",
    "          f.write(str(i)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data.loc[data.author_id_real.isin(bots),\"author_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle(\"../data/for_analysis/data0.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c498fff0e646d7d7e47743c6b62b2b1749eb16b128d528401df01971235654c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
